# Embeddings

Text-Embeddings sind numerische Darstellungen von Text, die es erm√∂glichen, semantische √Ñhnlichkeit zu messen. Dieser Leitfaden f√ºhrt in Embeddings, ihre Anwendungen und die Verwendung von Embedding-Modellen f√ºr Aufgaben wie Suche, Empfehlungen und Anomalieerkennung ein.

---

## Vor der Implementierung von Embeddings

Bei der Auswahl eines Embeddings-Anbieters gibt es mehrere Faktoren, die Sie je nach Ihren Bed√ºrfnissen und Pr√§ferenzen ber√ºcksichtigen k√∂nnen:

- Datensatzgr√∂√üe & Dom√§nenspezifit√§t: Gr√∂√üe des Modell-Trainingsdatensatzes und seine Relevanz f√ºr die Dom√§ne, die Sie einbetten m√∂chten. Gr√∂√üere oder dom√§nenspezifischere Daten erzeugen im Allgemeinen bessere dom√§neninterne Embeddings
- Inferenz-Performance: Geschwindigkeit der Embedding-Suche und End-to-End-Latenz. Dies ist eine besonders wichtige √úberlegung f√ºr gro√ü angelegte Produktionsbereitstellungen
- Anpassung: Optionen f√ºr fortgesetztes Training auf privaten Daten oder Spezialisierung von Modellen f√ºr sehr spezifische Dom√§nen. Dies kann die Leistung bei einzigartigen Vokabularen verbessern

## Wie man Embeddings mit Anthropic erh√§lt

Anthropic bietet kein eigenes Embedding-Modell an. Ein Embeddings-Anbieter, der eine gro√üe Vielfalt an Optionen und F√§higkeiten hat, die alle oben genannten √úberlegungen umfassen, ist Voyage AI.

Voyage AI erstellt hochmoderne Embedding-Modelle und bietet angepasste Modelle f√ºr spezifische Industriedom√§nen wie Finanzen und Gesundheitswesen oder ma√ügeschneiderte fein abgestimmte Modelle f√ºr einzelne Kunden.

Der Rest dieses Leitfadens ist f√ºr Voyage AI, aber wir ermutigen Sie, eine Vielzahl von Embeddings-Anbietern zu bewerten, um die beste L√∂sung f√ºr Ihren spezifischen Anwendungsfall zu finden.

## Verf√ºgbare Modelle

Voyage empfiehlt die Verwendung der folgenden Text-Embedding-Modelle:

| Modell | Kontextl√§nge | Embedding-Dimension | Beschreibung |
| --- | --- | --- | --- |
| `voyage-3-large` | 32.000 | 1024 (Standard), 256, 512, 2048 | Die beste allgemeine und mehrsprachige Retrieval-Qualit√§t. Siehe [Blog-Post](https://blog.voyageai.com/2025/01/07/voyage-3-large/) f√ºr Details. |
| `voyage-3.5` | 32.000 | 1024 (Standard), 256, 512, 2048 | Optimiert f√ºr allgemeine und mehrsprachige Retrieval-Qualit√§t. Siehe [Blog-Post](https://blog.voyageai.com/2025/05/20/voyage-3-5/) f√ºr Details. |
| `voyage-3.5-lite` | 32.000 | 1024 (Standard), 256, 512, 2048 | Optimiert f√ºr Latenz und Kosten. Siehe [Blog-Post](https://blog.voyageai.com/2025/05/20/voyage-3-5/) f√ºr Details. |
| `voyage-code-3` | 32.000 | 1024 (Standard), 256, 512, 2048 | Optimiert f√ºr **Code**-Retrieval. Siehe [Blog-Post](https://blog.voyageai.com/2024/12/04/voyage-code-3/) f√ºr Details. |
| `voyage-finance-2` | 32.000 | 1024 | Optimiert f√ºr **Finanz**-Retrieval und RAG. Siehe [Blog-Post](https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/) f√ºr Details. |
| `voyage-law-2` | 16.000 | 1024 | Optimiert f√ºr **rechtliche** und **lange Kontext**-Retrieval und RAG. Auch verbesserte Leistung in allen Dom√§nen. Siehe [Blog-Post](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/) f√ºr Details. |

Zus√§tzlich werden die folgenden multimodalen Embedding-Modelle empfohlen:

| Modell | Kontextl√§nge | Embedding-Dimension | Beschreibung |
| --- | --- | --- | --- |
| `voyage-multimodal-3` | 32000 | 1024 | Reichhaltiges multimodales Embedding-Modell, das verschachtelten Text und inhaltsreiche Bilder wie Screenshots von PDFs, Folien, Tabellen, Abbildungen und mehr vektorisieren kann. Siehe [Blog-Post](https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/) f√ºr Details. |

Ben√∂tigen Sie Hilfe bei der Entscheidung, welches Text-Embedding-Modell Sie verwenden sollen? Schauen Sie sich die [FAQ](https://docs.voyageai.com/docs/faq#what-embedding-models-are-available-and-which-one-should-i-use&ref=anthropic) an.

## Erste Schritte mit Voyage AI

Um auf Voyage-Embeddings zuzugreifen:

1. Registrieren Sie sich auf der Website von Voyage AI
2. Erhalten Sie einen API-Schl√ºssel
3. Setzen Sie den API-Schl√ºssel als Umgebungsvariable f√ºr die Bequemlichkeit:

```bash
export VOYAGE_API_KEY="<your secret key>"
```

Sie k√∂nnen die Embeddings entweder mit dem offiziellen [`voyageai` Python-Paket](https://github.com/voyage-ai/voyageai-python) oder HTTP-Anfragen erhalten, wie unten beschrieben.

### Voyage Python-Bibliothek

Das `voyageai`-Paket kann mit dem folgenden Befehl installiert werden:

```bash
pip install -U voyageai
```

Dann k√∂nnen Sie ein Client-Objekt erstellen und es verwenden, um Ihre Texte einzubetten:

```python
import voyageai

vo = voyageai.Client()
# Dies wird automatisch die Umgebungsvariable VOYAGE_API_KEY verwenden.
# Alternativ k√∂nnen Sie vo = voyageai.Client(api_key="<your secret key>") verwenden

texts = ["Beispieltext 1", "Beispieltext 2"]

result = vo.embed(texts, model="voyage-3.5", input_type="document")
print(result.embeddings[0])
print(result.embeddings[1])
```

`result.embeddings` wird eine Liste von zwei Embedding-Vektoren sein, die jeweils 1024 Gleitkommazahlen enthalten. Nach dem Ausf√ºhren des obigen Codes werden die beiden Embeddings auf dem Bildschirm ausgegeben:

```
[-0.013131560757756233, 0.019828535616397858, ...]   # embedding f√ºr "Beispieltext 1"
[-0.0069352793507277966, 0.020878976210951805, ...]  # embedding f√ºr "Beispieltext 2"
```

Beim Erstellen der Embeddings k√∂nnen Sie einige andere Argumente f√ºr die `embed()`-Funktion angeben.

F√ºr weitere Informationen zum Voyage Python-Paket siehe [die Voyage-Dokumentation](https://docs.voyageai.com/docs/embeddings#python-api).

### Voyage HTTP API

Sie k√∂nnen auch Embeddings erhalten, indem Sie die Voyage HTTP API anfordern. Zum Beispiel k√∂nnen Sie eine HTTP-Anfrage √ºber den `curl`-Befehl in einem Terminal senden:

```bash
curl https://api.voyageai.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $VOYAGE_API_KEY" \
  -d '{
    "input": ["Beispieltext 1", "Beispieltext 2"],
    "model": "voyage-3.5"
  }'
```

Die Antwort, die Sie erhalten w√ºrden, ist ein JSON-Objekt, das die Embeddings und die Token-Nutzung enth√§lt:

```json
{
  "object": "list",
  "data": [
    {
      "embedding": [-0.013131560757756233, 0.019828535616397858, ...],
      "index": 0
    },
    {
      "embedding": [-0.0069352793507277966, 0.020878976210951805, ...],
      "index": 1
    }
  ],
  "model": "voyage-3.5",
  "usage": {
    "total_tokens": 10
  }
}

```

F√ºr weitere Informationen zur Voyage HTTP API siehe [die Voyage-Dokumentation](https://docs.voyageai.com/reference/embeddings-api).

### AWS Marketplace

Voyage-Embeddings sind auf dem [AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-snt4gb6fd7ljg) verf√ºgbar. Anweisungen f√ºr den Zugang zu Voyage auf AWS sind [hier](https://docs.voyageai.com/docs/aws-marketplace-model-package?ref=anthropic) verf√ºgbar.

## Schnellstart-Beispiel

Jetzt, da wir wissen, wie man Embeddings erh√§lt, schauen wir uns ein kurzes Beispiel an.

Angenommen, wir haben ein kleines Korpus von sechs Dokumenten, aus denen wir abrufen k√∂nnen

```python
documents = [
    "Die Mittelmeerdi√§t betont Fisch, Oliven√∂l und Gem√ºse, von denen angenommen wird, dass sie chronische Krankheiten reduzieren.",
    "Photosynthese in Pflanzen wandelt Lichtenergie in Glukose um und produziert essentiellen Sauerstoff.",
    "Innovationen des 20. Jahrhunderts, von Radios bis zu Smartphones, konzentrierten sich auf elektronische Fortschritte.",
    "Fl√ºsse liefern Wasser, Bew√§sserung und Lebensraum f√ºr Wasserarten, die f√ºr √ñkosysteme lebenswichtig sind.",
    "Apples Telefonkonferenz zur Diskussion der Ergebnisse des vierten Gesch√§ftsquartals und Gesch√§ftsupdates ist f√ºr Donnerstag, den 2. November 2023 um 14:00 Uhr PT / 17:00 Uhr ET geplant.",
    "Shakespeares Werke, wie 'Hamlet' und 'Ein Sommernachtstraum', bestehen in der Literatur fort."
]

```

Wir werden zuerst Voyage verwenden, um jedes von ihnen in einen Embedding-Vektor umzuwandeln

```python
import voyageai

vo = voyageai.Client()

# Einbetten der Dokumente
doc_embds = vo.embed(
    documents, model="voyage-3.5", input_type="document"
).embeddings
```

Die Embeddings erm√∂glichen es uns, semantische Suche / Retrieval im Vektorraum durchzuf√ºhren. Bei einer Beispielabfrage,

```python
query = "Wann ist Apples Telefonkonferenz geplant?"
```

wandeln wir sie in ein Embedding um und f√ºhren eine N√§chste-Nachbarn-Suche durch, um das relevanteste Dokument basierend auf der Entfernung im Embedding-Raum zu finden.

```python
import numpy as np

# Einbetten der Abfrage
query_embd = vo.embed(
    [query], model="voyage-3.5", input_type="query"
).embeddings[0]

# Berechnen der √Ñhnlichkeit
# Voyage-Embeddings sind auf L√§nge 1 normalisiert, daher sind Skalarprodukt
# und Kosinus-√Ñhnlichkeit dasselbe.
similarities = np.dot(doc_embds, query_embd)

retrieved_id = np.argmax(similarities)
print(documents[retrieved_id])
```

Beachten Sie, dass wir `input_type="document"` und `input_type="query"` f√ºr das Einbetten des Dokuments bzw. der Abfrage verwenden. Weitere Spezifikationen finden Sie [hier](/docs/de/build-with-claude/embeddings#voyage-python-package).

Die Ausgabe w√§re das 5. Dokument, das tats√§chlich am relevantesten f√ºr die Abfrage ist:

```
Apples Telefonkonferenz zur Diskussion der Ergebnisse des vierten Gesch√§ftsquartals und Gesch√§ftsupdates ist f√ºr Donnerstag, den 2. November 2023 um 14:00 Uhr PT / 17:00 Uhr ET geplant.
```

Wenn Sie nach einem detaillierten Satz von Kochb√ºchern suchen, wie man RAG mit Embeddings macht, einschlie√ülich Vektordatenbanken, schauen Sie sich unser [RAG-Kochbuch](https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/Pinecone/rag_using_pinecone.ipynb) an.

## FAQ

  <section title="Warum haben Voyage-Embeddings √ºberlegene Qualit√§t?">

    Embedding-Modelle basieren auf leistungsstarken neuronalen Netzwerken, um semantischen Kontext zu erfassen und zu komprimieren, √§hnlich wie generative Modelle. Voyages Team erfahrener KI-Forscher optimiert jede Komponente des Embedding-Prozesses, einschlie√ülich:
    - Modellarchitektur 
    - Datensammlung
    - Verlustfunktionen
    - Optimierer-Auswahl

    Erfahren Sie mehr √ºber Voyages technischen Ansatz in ihrem [Blog](https://blog.voyageai.com/).
  
</section>

  <section title="Welche Embedding-Modelle sind verf√ºgbar und welches sollte ich verwenden?">

    F√ºr allgemeine Embedding-Zwecke empfehlen wir:
    - `voyage-3-large`: Beste Qualit√§t
    - `voyage-3.5-lite`: Niedrigste Latenz und Kosten
    - `voyage-3.5`: Ausgewogene Leistung mit √ºberlegener Retrieval-Qualit√§t zu einem wettbewerbsf√§higen Preis
    
    F√ºr Retrieval verwenden Sie den `input_type`-Parameter, um anzugeben, ob der Text eine Abfrage oder ein Dokumenttyp ist.

    Dom√§nenspezifische Modelle:

    - Rechtliche Aufgaben: `voyage-law-2`
    - Code und Programmierdokumentation: `voyage-code-3`
    - Finanzbezogene Aufgaben: `voyage-finance-2`
  
</section>

  <section title="Welche √Ñhnlichkeitsfunktion sollte ich verwenden?">

    Sie k√∂nnen Voyage-Embeddings mit Skalarprodukt-√Ñhnlichkeit, Kosinus-√Ñhnlichkeit oder euklidischer Entfernung verwenden. Eine Erkl√§rung zur Embedding-√Ñhnlichkeit finden Sie [hier](https://www.pinecone.io/learn/vector-similarity/).

    Voyage AI-Embeddings sind auf L√§nge 1 normalisiert, was bedeutet, dass:

    - Kosinus-√Ñhnlichkeit ist √§quivalent zur Skalarprodukt-√Ñhnlichkeit, w√§hrend letztere schneller berechnet werden kann.
    - Kosinus-√Ñhnlichkeit und euklidische Entfernung f√ºhren zu identischen Rankings.
  
</section>

  <section title="Was ist die Beziehung zwischen Zeichen, W√∂rtern und Token?">

    Bitte sehen Sie sich diese [Seite](https://docs.voyageai.com/docs/tokenization?ref=anthropic) an.
  
</section>

  <section title="Wann und wie sollte ich den input_type-Parameter verwenden?">

    F√ºr alle Retrieval-Aufgaben und Anwendungsf√§lle (z.B. RAG) empfehlen wir, dass der `input_type`-Parameter verwendet wird, um anzugeben, ob der Eingabetext eine Abfrage oder ein Dokument ist. Lassen Sie `input_type` nicht weg oder setzen Sie `input_type=None`. Die Angabe, ob Eingabetext eine Abfrage oder ein Dokument ist, kann bessere dichte Vektordarstellungen f√ºr Retrieval erstellen, was zu besserer Retrieval-Qualit√§t f√ºhren kann.

    Bei Verwendung des `input_type`-Parameters werden spezielle Prompts dem Eingabetext vor dem Einbetten vorangestellt. Spezifisch:

    > üìò **Prompts im Zusammenhang mit `input_type`**
    > 
    > - F√ºr eine Abfrage lautet der Prompt "Represent the query for retrieving supporting documents: ".
    > - F√ºr ein Dokument lautet der Prompt "Represent the document for retrieval: ".
    > - Beispiel
    >     - Wenn `input_type="query"`, wird eine Abfrage wie "Wann ist Apples Telefonkonferenz geplant?" zu "**Represent the query for retrieving supporting documents:** Wann ist Apples Telefonkonferenz geplant?"
    >     - Wenn `input_type="document"`, wird eine Abfrage wie "Apples Telefonkonferenz zur Diskussion der Ergebnisse des vierten Gesch√§ftsquartals und Gesch√§ftsupdates ist f√ºr Donnerstag, den 2. November 2023 um 14:00 Uhr PT / 17:00 Uhr ET geplant." zu "**Represent the document for retrieval:** Apples Telefonkonferenz zur Diskussion der Ergebnisse des vierten Gesch√§ftsquartals und Gesch√§ftsupdates ist f√ºr Donnerstag, den 2. November 2023 um 14:00 Uhr PT / 17:00 Uhr ET geplant."

    `voyage-large-2-instruct`, wie der Name schon sagt, ist darauf trainiert, auf zus√§tzliche Anweisungen zu reagieren, die dem Eingabetext vorangestellt werden. F√ºr Klassifikation, Clustering oder andere [MTEB](https://huggingface.co/mteb)-Unteraufgaben verwenden Sie bitte die Anweisungen [hier](https://github.com/voyage-ai/voyage-large-2-instruct).
  
</section>

  <section title="Welche Quantisierungsoptionen sind verf√ºgbar?">

    Quantisierung in Embeddings wandelt hochpr√§zise Werte, wie 32-Bit-Einzelpr√§zisions-Gleitkommazahlen, in niedrigere Pr√§zisionsformate wie 8-Bit-Ganzzahlen oder 1-Bit-Bin√§rwerte um, wodurch Speicher, Arbeitsspeicher und Kosten um das 4-fache bzw. 32-fache reduziert werden. Unterst√ºtzte Voyage-Modelle erm√∂glichen Quantisierung durch Angabe des Ausgabedatentyps mit dem `output_dtype`-Parameter:

    - `float`: Jedes zur√ºckgegebene Embedding ist eine Liste von 32-Bit (4-Byte) Einzelpr√§zisions-Gleitkommazahlen. Dies ist der Standard und bietet die h√∂chste Pr√§zision / Retrieval-Genauigkeit.
    - `int8` und `uint8`: Jedes zur√ºckgegebene Embedding ist eine Liste von 8-Bit (1-Byte) Ganzzahlen im Bereich von -128 bis 127 bzw. 0 bis 255.
    - `binary` und `ubinary`: Jedes zur√ºckgegebene Embedding ist eine Liste von 8-Bit-Ganzzahlen, die bit-gepackte, quantisierte Ein-Bit-Embedding-Werte darstellen: `int8` f√ºr `binary` und `uint8` f√ºr `ubinary`. Die L√§nge der zur√ºckgegebenen Liste von Ganzzahlen betr√§gt 1/8 der tats√§chlichen Dimension des Embeddings. Der bin√§re Typ verwendet die Offset-Bin√§r-Methode, √ºber die Sie mehr in der FAQ unten erfahren k√∂nnen.

    > **Beispiel f√ºr bin√§re Quantisierung**
    > 
    > Betrachten Sie die folgenden acht Embedding-Werte: -0.03955078, 0.006214142, -0.07446289, -0.039001465, 0.0046463013, 0.00030612946, -0.08496094 und 0.03994751. Mit bin√§rer Quantisierung werden Werte kleiner oder gleich null zu einer bin√§ren Null quantisiert und positive Werte zu einer bin√§ren Eins, was zu der folgenden bin√§ren Sequenz f√ºhrt: 0, 1, 0, 0, 1, 1, 0, 1. Diese acht Bits werden dann in eine einzige 8-Bit-Ganzzahl gepackt, 01001101 (mit dem linkesten Bit als dem signifikantesten Bit).
    >   - `ubinary`: Die bin√§re Sequenz wird direkt umgewandelt und als vorzeichenlose Ganzzahl (`uint8`) 77 dargestellt.
    >   - `binary`: Die bin√§re Sequenz wird als vorzeichenbehaftete Ganzzahl (`int8`) -51 dargestellt, berechnet mit der Offset-Bin√§r-Methode (77 - 128 = -51).
  
</section>

  <section title="Wie kann ich Matryoshka-Embeddings k√ºrzen?">

    Matryoshka-Lernen erstellt Embeddings mit grob-zu-fein Darstellungen innerhalb eines einzigen Vektors. Voyage-Modelle, wie `voyage-code-3`, die mehrere Ausgabedimensionen unterst√ºtzen, generieren solche Matryoshka-Embeddings. Sie k√∂nnen diese Vektoren k√ºrzen, indem Sie die f√ºhrende Teilmenge von Dimensionen behalten. Zum Beispiel zeigt der folgende Python-Code, wie man 1024-dimensionale Vektoren auf 256 Dimensionen k√ºrzt:

    ```python
    import voyageai
    import numpy as np

    def embd_normalize(v: np.ndarray) -> np.ndarray:
         
        """
        Normalisiert die Zeilen eines 2D-numpy-Arrays zu Einheitsvektoren, indem jede Zeile durch ihre euklidische
        Norm geteilt wird. L√∂st einen ValueError aus, wenn eine Zeile eine Norm von null hat, um Division durch null zu verhindern.
        """
        row_norms = np.linalg.norm(v, axis=1, keepdims=True)
        if np.any(row_norms == 0):
            raise ValueError("Kann Zeilen mit einer Norm von null nicht normalisieren.")
        return v / row_norms


    vo = voyageai.Client()

    # Generiere voyage-code-3-Vektoren, die standardm√§√üig 1024-dimensionale Gleitkommazahlen sind
    embd = vo.embed(['Beispieltext 1', 'Beispieltext 2'], model='voyage-code-3').embeddings

    # Setze k√ºrzere Dimension
    short_dim = 256

    # Gr√∂√üe √§ndern und Vektoren auf k√ºrzere Dimension normalisieren
    resized_embd = embd_normalize(np.array(embd)[:, :short_dim]).tolist()
    ```
  
</section>

## Preise

Besuchen Sie Voyages [Preisseite](https://docs.voyageai.com/docs/pricing?ref=anthropic) f√ºr die aktuellsten Preisdetails.