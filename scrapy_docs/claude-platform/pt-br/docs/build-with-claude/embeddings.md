# Embeddings

Embeddings de texto s√£o representa√ß√µes num√©ricas de texto que permitem medir similaridade sem√¢ntica. Este guia apresenta embeddings, suas aplica√ß√µes e como usar modelos de embedding para tarefas como busca, recomenda√ß√µes e detec√ß√£o de anomalias.

---

## Antes de implementar embeddings

Ao selecionar um provedor de embeddings, h√° v√°rios fatores que voc√™ pode considerar dependendo de suas necessidades e prefer√™ncias:

- Tamanho do conjunto de dados e especificidade do dom√≠nio: tamanho do conjunto de dados de treinamento do modelo e sua relev√¢ncia para o dom√≠nio que voc√™ deseja incorporar. Dados maiores ou mais espec√≠ficos do dom√≠nio geralmente produzem melhores embeddings no dom√≠nio
- Desempenho de infer√™ncia: velocidade de busca de embedding e lat√™ncia de ponta a ponta. Esta √© uma considera√ß√£o particularmente importante para implanta√ß√µes de produ√ß√£o em larga escala
- Personaliza√ß√£o: op√ß√µes para treinamento continuado em dados privados, ou especializa√ß√£o de modelos para dom√≠nios muito espec√≠ficos. Isso pode melhorar o desempenho em vocabul√°rios √∫nicos

## Como obter embeddings com Anthropic

A Anthropic n√£o oferece seu pr√≥prio modelo de embedding. Um provedor de embeddings que tem uma ampla variedade de op√ß√µes e capacidades abrangendo todas as considera√ß√µes acima √© a Voyage AI.

A Voyage AI cria modelos de embedding de √∫ltima gera√ß√£o e oferece modelos personalizados para dom√≠nios industriais espec√≠ficos como finan√ßas e sa√∫de, ou modelos ajustados sob medida para clientes individuais.

O restante deste guia √© para Voyage AI, mas encorajamos voc√™ a avaliar uma variedade de fornecedores de embeddings para encontrar o melhor ajuste para seu caso de uso espec√≠fico.

## Modelos Dispon√≠veis

A Voyage recomenda usar os seguintes modelos de embedding de texto:

| Modelo | Comprimento do Contexto | Dimens√£o do Embedding | Descri√ß√£o |
| --- | --- | --- | --- |
| `voyage-3-large` | 32,000 | 1024 (padr√£o), 256, 512, 2048 | A melhor qualidade de recupera√ß√£o geral e multil√≠ngue. Veja [post do blog](https://blog.voyageai.com/2025/01/07/voyage-3-large/) para detalhes. |
| `voyage-3.5` | 32,000 | 1024 (padr√£o), 256, 512, 2048 | Otimizado para qualidade de recupera√ß√£o geral e multil√≠ngue. Veja [post do blog](https://blog.voyageai.com/2025/05/20/voyage-3-5/) para detalhes. |
| `voyage-3.5-lite` | 32,000 | 1024 (padr√£o), 256, 512, 2048 | Otimizado para lat√™ncia e custo. Veja [post do blog](https://blog.voyageai.com/2025/05/20/voyage-3-5/) para detalhes. |
| `voyage-code-3` | 32,000 | 1024 (padr√£o), 256, 512, 2048 | Otimizado para recupera√ß√£o de **c√≥digo**. Veja [post do blog](https://blog.voyageai.com/2024/12/04/voyage-code-3/) para detalhes. |
| `voyage-finance-2` | 32,000 | 1024 | Otimizado para recupera√ß√£o e RAG de **finan√ßas**. Veja [post do blog](https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/) para detalhes. |
| `voyage-law-2` | 16,000 | 1024 | Otimizado para recupera√ß√£o e RAG **jur√≠dico** e de **contexto longo**. Tamb√©m melhorou o desempenho em todos os dom√≠nios. Veja [post do blog](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/) para detalhes. |

Al√©m disso, os seguintes modelos de embedding multimodal s√£o recomendados:

| Modelo | Comprimento do Contexto | Dimens√£o do Embedding | Descri√ß√£o |
| --- | --- | --- | --- |
| `voyage-multimodal-3` | 32000 | 1024 | Modelo de embedding multimodal rico que pode vetorizar texto intercalado e imagens ricas em conte√∫do, como capturas de tela de PDFs, slides, tabelas, figuras e muito mais. Veja [post do blog](https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/) para detalhes. |

Precisa de ajuda para decidir qual modelo de embedding de texto usar? Confira o [FAQ](https://docs.voyageai.com/docs/faq#what-embedding-models-are-available-and-which-one-should-i-use&ref=anthropic).

## Come√ßando com Voyage AI

Para acessar embeddings Voyage:

1. Inscreva-se no site da Voyage AI
2. Obtenha uma chave de API
3. Defina a chave de API como uma vari√°vel de ambiente para conveni√™ncia:

```bash
export VOYAGE_API_KEY="<sua chave secreta>"
```

Voc√™ pode obter os embeddings usando o [pacote Python oficial `voyageai`](https://github.com/voyage-ai/voyageai-python) ou solicita√ß√µes HTTP, conforme descrito abaixo.

### Biblioteca Python Voyage

O pacote `voyageai` pode ser instalado usando o seguinte comando:

```bash
pip install -U voyageai
```

Em seguida, voc√™ pode criar um objeto cliente e come√ßar a us√°-lo para incorporar seus textos:

```python
import voyageai

vo = voyageai.Client()
# Isso usar√° automaticamente a vari√°vel de ambiente VOYAGE_API_KEY.
# Alternativamente, voc√™ pode usar vo = voyageai.Client(api_key="<sua chave secreta>")

texts = ["Texto de exemplo 1", "Texto de exemplo 2"]

result = vo.embed(texts, model="voyage-3.5", input_type="document")
print(result.embeddings[0])
print(result.embeddings[1])
```

`result.embeddings` ser√° uma lista de dois vetores de embedding, cada um contendo 1024 n√∫meros de ponto flutuante. Ap√≥s executar o c√≥digo acima, os dois embeddings ser√£o impressos na tela:

```
[-0.013131560757756233, 0.019828535616397858, ...]   # embedding para "Texto de exemplo 1"
[-0.0069352793507277966, 0.020878976210951805, ...]  # embedding para "Texto de exemplo 2"
```

Ao criar os embeddings, voc√™ pode especificar alguns outros argumentos para a fun√ß√£o `embed()`.

Para mais informa√ß√µes sobre o pacote python Voyage, veja [a documenta√ß√£o Voyage](https://docs.voyageai.com/docs/embeddings#python-api).

### API HTTP Voyage

Voc√™ tamb√©m pode obter embeddings solicitando a API HTTP Voyage. Por exemplo, voc√™ pode enviar uma solicita√ß√£o HTTP atrav√©s do comando `curl` em um terminal:

```bash
curl https://api.voyageai.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $VOYAGE_API_KEY" \
  -d '{
    "input": ["Texto de exemplo 1", "Texto de exemplo 2"],
    "model": "voyage-3.5"
  }'
```

A resposta que voc√™ obteria √© um objeto JSON contendo os embeddings e o uso de tokens:

```json
{
  "object": "list",
  "data": [
    {
      "embedding": [-0.013131560757756233, 0.019828535616397858, ...],
      "index": 0
    },
    {
      "embedding": [-0.0069352793507277966, 0.020878976210951805, ...],
      "index": 1
    }
  ],
  "model": "voyage-3.5",
  "usage": {
    "total_tokens": 10
  }
}

```

Para mais informa√ß√µes sobre a API HTTP Voyage, veja [a documenta√ß√£o Voyage](https://docs.voyageai.com/reference/embeddings-api).

### AWS Marketplace

Embeddings Voyage est√£o dispon√≠veis no [AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-snt4gb6fd7ljg). Instru√ß√µes para acessar Voyage na AWS est√£o dispon√≠veis [aqui](https://docs.voyageai.com/docs/aws-marketplace-model-package?ref=anthropic).

## Exemplo de in√≠cio r√°pido

Agora que sabemos como obter embeddings, vamos ver um exemplo breve.

Suponha que temos um pequeno corpus de seis documentos para recuperar

```python
documents = [
    "A dieta mediterr√¢nea enfatiza peixe, azeite de oliva e vegetais, acreditada para reduzir doen√ßas cr√¥nicas.",
    "A fotoss√≠ntese nas plantas converte energia luminosa em glicose e produz oxig√™nio essencial.",
    "Inova√ß√µes do s√©culo 20, de r√°dios a smartphones, centraram-se em avan√ßos eletr√¥nicos.",
    "Rios fornecem √°gua, irriga√ß√£o e habitat para esp√©cies aqu√°ticas, vitais para ecossistemas.",
    "A confer√™ncia telef√¥nica da Apple para discutir os resultados do quarto trimestre fiscal e atualiza√ß√µes de neg√≥cios est√° agendada para quinta-feira, 2 de novembro de 2023 √†s 14:00 PT / 17:00 ET.",
    "As obras de Shakespeare, como 'Hamlet' e 'Sonho de uma Noite de Ver√£o,' perduram na literatura."
]

```

Primeiro usaremos Voyage para converter cada um deles em um vetor de embedding

```python
import voyageai

vo = voyageai.Client()

# Incorporar os documentos
doc_embds = vo.embed(
    documents, model="voyage-3.5", input_type="document"
).embeddings
```

Os embeddings nos permitir√£o fazer busca sem√¢ntica / recupera√ß√£o no espa√ßo vetorial. Dada uma consulta de exemplo,

```python
query = "Quando est√° agendada a confer√™ncia telef√¥nica da Apple?"
```

n√≥s a convertemos em um embedding, e conduzimos uma busca de vizinho mais pr√≥ximo para encontrar o documento mais relevante baseado na dist√¢ncia no espa√ßo de embedding.

```python
import numpy as np

# Incorporar a consulta
query_embd = vo.embed(
    [query], model="voyage-3.5", input_type="query"
).embeddings[0]

# Calcular a similaridade
# Embeddings Voyage s√£o normalizados para comprimento 1, portanto produto escalar
# e similaridade de cosseno s√£o os mesmos.
similarities = np.dot(doc_embds, query_embd)

retrieved_id = np.argmax(similarities)
print(documents[retrieved_id])
```

Note que usamos `input_type="document"` e `input_type="query"` para incorporar o documento e consulta, respectivamente. Mais especifica√ß√£o pode ser encontrada [aqui](/docs/pt-BR/build-with-claude/embeddings#voyage-python-package).

A sa√≠da seria o 5¬∫ documento, que √© de fato o mais relevante para a consulta:

```
A confer√™ncia telef√¥nica da Apple para discutir os resultados do quarto trimestre fiscal e atualiza√ß√µes de neg√≥cios est√° agendada para quinta-feira, 2 de novembro de 2023 √†s 14:00 PT / 17:00 ET.
```

Se voc√™ est√° procurando por um conjunto detalhado de livros de receitas sobre como fazer RAG com embeddings, incluindo bancos de dados vetoriais, confira nosso [livro de receitas RAG](https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/Pinecone/rag_using_pinecone.ipynb).

## FAQ

  <section title="Por que os embeddings Voyage t√™m qualidade superior?">

    Modelos de embedding dependem de redes neurais poderosas para capturar e comprimir contexto sem√¢ntico, similar a modelos generativos. A equipe de pesquisadores de IA experientes da Voyage otimiza cada componente do processo de embedding, incluindo:
    - Arquitetura do modelo
    - Coleta de dados
    - Fun√ß√µes de perda
    - Sele√ß√£o de otimizador

    Saiba mais sobre a abordagem t√©cnica da Voyage em seu [blog](https://blog.voyageai.com/).
  
</section>

  <section title="Quais modelos de embedding est√£o dispon√≠veis e qual devo usar?">

    Para embedding de prop√≥sito geral, recomendamos:
    - `voyage-3-large`: Melhor qualidade
    - `voyage-3.5-lite`: Menor lat√™ncia e custo
    - `voyage-3.5`: Desempenho equilibrado com qualidade de recupera√ß√£o superior a um ponto de pre√ßo competitivo
    
    Para recupera√ß√£o, use o par√¢metro `input_type` para especificar se o texto √© um tipo de consulta ou documento.

    Modelos espec√≠ficos de dom√≠nio:

    - Tarefas jur√≠dicas: `voyage-law-2`
    - C√≥digo e documenta√ß√£o de programa√ß√£o: `voyage-code-3`
    - Tarefas relacionadas a finan√ßas: `voyage-finance-2`
  
</section>

  <section title="Qual fun√ß√£o de similaridade devo usar?">

    Voc√™ pode usar embeddings Voyage com similaridade de produto escalar, similaridade de cosseno ou dist√¢ncia euclidiana. Uma explica√ß√£o sobre similaridade de embedding pode ser encontrada [aqui](https://www.pinecone.io/learn/vector-similarity/).

    Embeddings Voyage AI s√£o normalizados para comprimento 1, o que significa que:

    - Similaridade de cosseno √© equivalente √† similaridade de produto escalar, enquanto a √∫ltima pode ser computada mais rapidamente.
    - Similaridade de cosseno e dist√¢ncia euclidiana resultar√£o em classifica√ß√µes id√™nticas.
  
</section>

  <section title="Qual √© a rela√ß√£o entre caracteres, palavras e tokens?">

    Por favor, veja esta [p√°gina](https://docs.voyageai.com/docs/tokenization?ref=anthropic).
  
</section>

  <section title="Quando e como devo usar o par√¢metro input_type?">

    Para todas as tarefas de recupera√ß√£o e casos de uso (por exemplo, RAG), recomendamos que o par√¢metro `input_type` seja usado para especificar se o texto de entrada √© uma consulta ou documento. N√£o omita `input_type` ou defina `input_type=None`. Especificar se o texto de entrada √© uma consulta ou documento pode criar melhores representa√ß√µes de vetor denso para recupera√ß√£o, o que pode levar a melhor qualidade de recupera√ß√£o.

    Ao usar o par√¢metro `input_type`, prompts especiais s√£o anexados ao texto de entrada antes da incorpora√ß√£o. Especificamente:

    > üìò **Prompts associados com `input_type`**
    > 
    > - Para uma consulta, o prompt √© "Represente a consulta para recuperar documentos de apoio: ".
    > - Para um documento, o prompt √© "Represente o documento para recupera√ß√£o: ".
    > - Exemplo
    >     - Quando `input_type="query"`, uma consulta como "Quando est√° agendada a confer√™ncia telef√¥nica da Apple?" se tornar√° "**Represente a consulta para recuperar documentos de apoio:** Quando est√° agendada a confer√™ncia telef√¥nica da Apple?"
    >     - Quando `input_type="document"`, uma consulta como "A confer√™ncia telef√¥nica da Apple para discutir os resultados do quarto trimestre fiscal e atualiza√ß√µes de neg√≥cios est√° agendada para quinta-feira, 2 de novembro de 2023 √†s 14:00 PT / 17:00 ET." se tornar√° "**Represente o documento para recupera√ß√£o:** A confer√™ncia telef√¥nica da Apple para discutir os resultados do quarto trimestre fiscal e atualiza√ß√µes de neg√≥cios est√° agendada para quinta-feira, 2 de novembro de 2023 √†s 14:00 PT / 17:00 ET."

    `voyage-large-2-instruct`, como o nome sugere, √© treinado para ser responsivo a instru√ß√µes adicionais que s√£o anexadas ao texto de entrada. Para classifica√ß√£o, agrupamento ou outras subtarefas [MTEB](https://huggingface.co/mteb), por favor use as instru√ß√µes [aqui](https://github.com/voyage-ai/voyage-large-2-instruct).
  
</section>

  <section title="Quais op√ß√µes de quantiza√ß√£o est√£o dispon√≠veis?">

    Quantiza√ß√£o em embeddings converte valores de alta precis√£o, como n√∫meros de ponto flutuante de precis√£o simples de 32 bits, para formatos de menor precis√£o como inteiros de 8 bits ou valores bin√°rios de 1 bit, reduzindo armazenamento, mem√≥ria e custos em 4x e 32x, respectivamente. Modelos Voyage suportados habilitam quantiza√ß√£o especificando o tipo de dados de sa√≠da com o par√¢metro `output_dtype`:

    - `float`: Cada embedding retornado √© uma lista de n√∫meros de ponto flutuante de precis√£o simples de 32 bits (4 bytes). Este √© o padr√£o e fornece a maior precis√£o / precis√£o de recupera√ß√£o.
    - `int8` e `uint8`: Cada embedding retornado √© uma lista de inteiros de 8 bits (1 byte) variando de -128 a 127 e 0 a 255, respectivamente.
    - `binary` e `ubinary`: Cada embedding retornado √© uma lista de inteiros de 8 bits que representam valores de embedding quantizados de um bit empacotados em bits: `int8` para `binary` e `uint8` para `ubinary`. O comprimento da lista retornada de inteiros √© 1/8 da dimens√£o real do embedding. O tipo bin√°rio usa o m√©todo bin√°rio de deslocamento, sobre o qual voc√™ pode aprender mais no FAQ abaixo.

    > **Exemplo de quantiza√ß√£o bin√°ria**
    > 
    > Considere os seguintes oito valores de embedding: -0.03955078, 0.006214142, -0.07446289, -0.039001465, 0.0046463013, 0.00030612946, -0.08496094, e 0.03994751. Com quantiza√ß√£o bin√°ria, valores menores ou iguais a zero ser√£o quantizados para um zero bin√°rio, e valores positivos para um um bin√°rio, resultando na seguinte sequ√™ncia bin√°ria: 0, 1, 0, 0, 1, 1, 0, 1. Estes oito bits s√£o ent√£o empacotados em um √∫nico inteiro de 8 bits, 01001101 (com o bit mais √† esquerda como o bit mais significativo).
    >   - `ubinary`: A sequ√™ncia bin√°ria √© diretamente convertida e representada como o inteiro sem sinal (`uint8`) 77.
    >   - `binary`: A sequ√™ncia bin√°ria √© representada como o inteiro com sinal (`int8`) -51, calculado usando o m√©todo bin√°rio de deslocamento (77 - 128 = -51).
  
</section>

  <section title="Como posso truncar embeddings Matryoshka?">

    Aprendizado Matryoshka cria embeddings com representa√ß√µes grossas a finas dentro de um √∫nico vetor. Modelos Voyage, como `voyage-code-3`, que suportam m√∫ltiplas dimens√µes de sa√≠da geram tais embeddings Matryoshka. Voc√™ pode truncar esses vetores mantendo o subconjunto principal de dimens√µes. Por exemplo, o seguinte c√≥digo Python demonstra como truncar vetores de 1024 dimens√µes para 256 dimens√µes:

    ```python
    import voyageai
    import numpy as np

    def embd_normalize(v: np.ndarray) -> np.ndarray:
        """
        Normalizar as linhas de um array numpy 2D para vetores unit√°rios dividindo cada linha por sua
        norma euclidiana. Levanta um ValueError se qualquer linha tiver uma norma de zero para prevenir divis√£o por zero.
        """
        row_norms = np.linalg.norm(v, axis=1, keepdims=True)
        if np.any(row_norms == 0):
            raise ValueError("N√£o √© poss√≠vel normalizar linhas com uma norma de zero.")
        return v / row_norms


    vo = voyageai.Client()

    # Gerar vetores voyage-code-3, que por padr√£o s√£o n√∫meros de ponto flutuante de 1024 dimens√µes
    embd = vo.embed(['Texto de exemplo 1', 'Texto de exemplo 2'], model='voyage-code-3').embeddings

    # Definir dimens√£o mais curta
    short_dim = 256

    # Redimensionar e normalizar vetores para dimens√£o mais curta
    resized_embd = embd_normalize(np.array(embd)[:, :short_dim]).tolist()
    ```
  
</section>

## Pre√ßos

Visite a [p√°gina de pre√ßos](https://docs.voyageai.com/docs/pricing?ref=anthropic) da Voyage para os detalhes de pre√ßos mais atualizados.