# レイテンシの削減

モデルがプロンプトを処理し、出力を生成するまでの時間を短縮する方法について説明します。

---

レイテンシとは、モデルがプロンプトを処理し、出力を生成するまでにかかる時間のことです。レイテンシは、モデルのサイズ、プロンプトの複雑さ、モデルと相互作用ポイントをサポートする基盤インフラストラクチャなど、さまざまな要因によって影響を受ける可能性があります。

<Note>
モデルやプロンプトの制約なしに適切に動作するプロンプトを最初に設計し、その後でレイテンシ削減戦略を試すことが常に良い方法です。レイテンシの削減を早急に試みると、最高のパフォーマンスがどのようなものかを発見することを妨げる可能性があります。
</Note>

---

## レイテンシの測定方法

レイテンシについて議論する際、いくつかの用語と測定値に遭遇する可能性があります：

- **ベースラインレイテンシ**：これは、入力および出力トークン毎秒を考慮せずに、モデルがプロンプトを処理し、応答を生成するのにかかる時間です。モデルの速度の一般的な概念を提供します。
- **最初のトークンまでの時間（TTFT）**：この指標は、プロンプトが送信されてから、モデルが応答の最初のトークンを生成するまでにかかる時間を測定します。ストリーミング（詳細は後述）を使用していて、ユーザーに応答性の高い体験を提供したい場合に特に関連します。

これらの用語のより詳細な理解については、[用語集](/docs/ja/about-claude/glossary)をご確認ください。

---

## レイテンシを削減する方法

### 1. 適切なモデルを選択する

レイテンシを削減する最も簡単な方法の一つは、使用ケースに適したモデルを選択することです。Anthropicは、異なる機能とパフォーマンス特性を持つ[さまざまなモデル](/docs/ja/about-claude/models/overview)を提供しています。特定の要件を考慮し、速度と出力品質の観点からニーズに最も適したモデルを選択してください。

速度が重要なアプリケーションでは、**Claude Haiku 4.5**が高い知能を維持しながら最速の応答時間を提供します：

```python
import anthropic

client = anthropic.Anthropic()

# 時間に敏感なアプリケーションには、Claude Haiku 4.5を使用
message = client.messages.create(
    model="claude-haiku-4-5",
    max_tokens=100,
    messages=[{
        "role": "user",
        "content": "この顧客フィードバックを2文で要約してください：[フィードバックテキスト]"
    }]
)
```

モデルメトリクスの詳細については、[モデル概要](/docs/ja/about-claude/models/overview)ページをご覧ください。

### 2. プロンプトと出力の長さを最適化する

高いパフォーマンスを維持しながら、入力プロンプトと期待される出力の両方でトークン数を最小限に抑えます。モデルが処理および生成するトークンが少ないほど、応答は速くなります。

プロンプトと出力を最適化するためのヒントをいくつか紹介します：

- **明確だが簡潔に**：プロンプトで意図を明確かつ簡潔に伝えることを目指します。不要な詳細や冗長な情報は避けますが、[claudeには文脈が不足している](/docs/ja/build-with-claude/prompt-engineering/be-clear-and-direct)ことを念頭に置き、指示が不明確な場合、意図された論理の飛躍を行わない可能性があることを覚えておいてください。
- **より短い応答を求める**：Claudeに直接簡潔であるよう求めます。Claude 3ファミリーのモデルは、以前の世代よりも操縦性が向上しています。Claudeが望ましくない長さを出力している場合は、Claudeに[おしゃべりを抑制する](/docs/ja/build-with-claude/prompt-engineering/be-clear-and-direct)よう求めてください。
  <Tip>LLMが単語ではなく[トークン](/docs/ja/about-claude/glossary#tokens)をカウントする方法のため、正確な単語数や単語数制限を求めることは、段落や文の数制限を求めるほど効果的な戦略ではありません。</Tip>
- **適切な出力制限を設定する**：`max_tokens`パラメータを使用して、生成される応答の最大長にハード制限を設定します。これにより、Claudeが過度に長い出力を生成することを防げます。
  > **注意**：応答が`max_tokens`トークンに達すると、応答は文の途中や単語の途中で切り取られる可能性があるため、これは後処理が必要な粗雑な技術であり、通常は答えが最初に来る多肢選択や短答問題に最も適しています。
- **temperatureを実験する**：`temperature`[パラメータ](/docs/ja/api/messages)は出力のランダム性を制御します。低い値（例：0.2）は、より焦点を絞った短い応答につながることがありますが、高い値（例：0.8）は、より多様だが潜在的により長い出力をもたらす可能性があります。

プロンプトの明確さ、出力品質、トークン数の適切なバランスを見つけるには、いくらかの実験が必要かもしれません。

### 3. ストリーミングを活用する

ストリーミングは、完全な出力が完成する前にモデルが応答の送信を開始できる機能です。これにより、ユーザーがモデルの出力をリアルタイムで見ることができるため、アプリケーションの知覚される応答性を大幅に改善できます。

ストリーミングが有効になっていると、モデルの出力が到着するにつれて処理し、ユーザーインターフェースを更新したり、他のタスクを並行して実行したりできます。これにより、ユーザー体験を大幅に向上させ、アプリケーションをよりインタラクティブで応答性の高いものにできます。

使用ケースにストリーミングを実装する方法については、[ストリーミングメッセージ](/docs/ja/build-with-claude/streaming)をご覧ください。