# Embeddings

Les embeddings de texte sont des repr√©sentations num√©riques de texte qui permettent de mesurer la similarit√© s√©mantique. Ce guide pr√©sente les embeddings, leurs applications, et comment utiliser les mod√®les d'embedding pour des t√¢ches comme la recherche, les recommandations, et la d√©tection d'anomalies.

---

## Avant d'impl√©menter les embeddings

Lors de la s√©lection d'un fournisseur d'embeddings, il y a plusieurs facteurs que vous pouvez consid√©rer selon vos besoins et pr√©f√©rences :

- Taille du jeu de donn√©es et sp√©cificit√© du domaine : taille du jeu de donn√©es d'entra√Ænement du mod√®le et sa pertinence par rapport au domaine que vous voulez int√©grer. Des donn√©es plus importantes ou plus sp√©cifiques au domaine produisent g√©n√©ralement de meilleurs embeddings dans le domaine
- Performance d'inf√©rence : vitesse de recherche d'embedding et latence de bout en bout. C'est une consid√©ration particuli√®rement importante pour les d√©ploiements de production √† grande √©chelle
- Personnalisation : options pour la formation continue sur des donn√©es priv√©es, ou la sp√©cialisation de mod√®les pour des domaines tr√®s sp√©cifiques. Cela peut am√©liorer les performances sur des vocabulaires uniques

## Comment obtenir des embeddings avec Anthropic

Anthropic n'offre pas son propre mod√®le d'embedding. Un fournisseur d'embeddings qui a une grande vari√©t√© d'options et de capacit√©s englobant toutes les consid√©rations ci-dessus est Voyage AI.

Voyage AI fabrique des mod√®les d'embedding de pointe et offre des mod√®les personnalis√©s pour des domaines industriels sp√©cifiques tels que la finance et la sant√©, ou des mod√®les sur mesure finement ajust√©s pour des clients individuels.

Le reste de ce guide concerne Voyage AI, mais nous vous encourageons √† √©valuer une vari√©t√© de fournisseurs d'embeddings pour trouver la meilleure solution pour votre cas d'usage sp√©cifique.

## Mod√®les disponibles

Voyage recommande d'utiliser les mod√®les d'embedding de texte suivants :

| Mod√®le | Longueur de contexte | Dimension d'embedding | Description |
| --- | --- | --- | --- |
| `voyage-3-large` | 32,000 | 1024 (par d√©faut), 256, 512, 2048 | La meilleure qualit√© de r√©cup√©ration g√©n√©raliste et multilingue. Voir [article de blog](https://blog.voyageai.com/2025/01/07/voyage-3-large/) pour les d√©tails. |
| `voyage-3.5` | 32,000 | 1024 (par d√©faut), 256, 512, 2048 | Optimis√© pour la qualit√© de r√©cup√©ration g√©n√©raliste et multilingue. Voir [article de blog](https://blog.voyageai.com/2025/05/20/voyage-3-5/) pour les d√©tails. |
| `voyage-3.5-lite` | 32,000 | 1024 (par d√©faut), 256, 512, 2048 | Optimis√© pour la latence et le co√ªt. Voir [article de blog](https://blog.voyageai.com/2025/05/20/voyage-3-5/) pour les d√©tails. |
| `voyage-code-3` | 32,000 | 1024 (par d√©faut), 256, 512, 2048 | Optimis√© pour la r√©cup√©ration de **code**. Voir [article de blog](https://blog.voyageai.com/2024/12/04/voyage-code-3/) pour les d√©tails. |
| `voyage-finance-2` | 32,000 | 1024 | Optimis√© pour la r√©cup√©ration et RAG en **finance**. Voir [article de blog](https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/) pour les d√©tails. |
| `voyage-law-2` | 16,000 | 1024 | Optimis√© pour la r√©cup√©ration et RAG **juridique** et **long contexte**. √âgalement am√©lioration des performances dans tous les domaines. Voir [article de blog](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/) pour les d√©tails. |

De plus, les mod√®les d'embedding multimodaux suivants sont recommand√©s :

| Mod√®le | Longueur de contexte | Dimension d'embedding | Description |
| --- | --- | --- | --- |
| `voyage-multimodal-3` | 32000 | 1024 | Mod√®le d'embedding multimodal riche qui peut vectoriser du texte entrelac√© et des images riches en contenu, telles que des captures d'√©cran de PDF, diapositives, tableaux, figures, et plus. Voir [article de blog](https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/) pour les d√©tails. |

Besoin d'aide pour d√©cider quel mod√®le d'embedding de texte utiliser ? Consultez la [FAQ](https://docs.voyageai.com/docs/faq#what-embedding-models-are-available-and-which-one-should-i-use&ref=anthropic).

## Commencer avec Voyage AI

Pour acc√©der aux embeddings Voyage :

1. Inscrivez-vous sur le site web de Voyage AI
2. Obtenez une cl√© API
3. D√©finissez la cl√© API comme variable d'environnement pour plus de commodit√© :

```bash
export VOYAGE_API_KEY="<votre cl√© secr√®te>"
```

Vous pouvez obtenir les embeddings soit en utilisant le package Python officiel [`voyageai`](https://github.com/voyage-ai/voyageai-python) ou les requ√™tes HTTP, comme d√©crit ci-dessous.

### Biblioth√®que Python Voyage

Le package `voyageai` peut √™tre install√© en utilisant la commande suivante :

```bash
pip install -U voyageai
```

Ensuite, vous pouvez cr√©er un objet client et commencer √† l'utiliser pour int√©grer vos textes :

```python
import voyageai

vo = voyageai.Client()
# Ceci utilisera automatiquement la variable d'environnement VOYAGE_API_KEY.
# Alternativement, vous pouvez utiliser vo = voyageai.Client(api_key="<votre cl√© secr√®te>")

texts = ["Texte d'exemple 1", "Texte d'exemple 2"]

result = vo.embed(texts, model="voyage-3.5", input_type="document")
print(result.embeddings[0])
print(result.embeddings[1])
```

`result.embeddings` sera une liste de deux vecteurs d'embedding, chacun contenant 1024 nombres √† virgule flottante. Apr√®s avoir ex√©cut√© le code ci-dessus, les deux embeddings seront affich√©s √† l'√©cran :

```
[-0.013131560757756233, 0.019828535616397858, ...]   # embedding pour "Texte d'exemple 1"
[-0.0069352793507277966, 0.020878976210951805, ...]  # embedding pour "Texte d'exemple 2"
```

Lors de la cr√©ation des embeddings, vous pouvez sp√©cifier quelques autres arguments √† la fonction `embed()`.

Pour plus d'informations sur le package python Voyage, voir [la documentation Voyage](https://docs.voyageai.com/docs/embeddings#python-api).

### API HTTP Voyage

Vous pouvez √©galement obtenir des embeddings en demandant l'API HTTP Voyage. Par exemple, vous pouvez envoyer une requ√™te HTTP via la commande `curl` dans un terminal :

```bash
curl https://api.voyageai.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $VOYAGE_API_KEY" \
  -d '{
    "input": ["Texte d'exemple 1", "Texte d'exemple 2"],
    "model": "voyage-3.5"
  }'
```

La r√©ponse que vous obtiendriez est un objet JSON contenant les embeddings et l'utilisation des tokens :

```json
{
  "object": "list",
  "data": [
    {
      "embedding": [-0.013131560757756233, 0.019828535616397858, ...],
      "index": 0
    },
    {
      "embedding": [-0.0069352793507277966, 0.020878976210951805, ...],
      "index": 1
    }
  ],
  "model": "voyage-3.5",
  "usage": {
    "total_tokens": 10
  }
}

```

Pour plus d'informations sur l'API HTTP Voyage, voir [la documentation Voyage](https://docs.voyageai.com/reference/embeddings-api).

### AWS Marketplace

Les embeddings Voyage sont disponibles sur [AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-snt4gb6fd7ljg). Les instructions pour acc√©der √† Voyage sur AWS sont disponibles [ici](https://docs.voyageai.com/docs/aws-marketplace-model-package?ref=anthropic).

## Exemple de d√©marrage rapide

Maintenant que nous savons comment obtenir des embeddings, voyons un bref exemple.

Supposons que nous ayons un petit corpus de six documents √† partir desquels r√©cup√©rer

```python
documents = [
    "Le r√©gime m√©diterran√©en met l'accent sur le poisson, l'huile d'olive et les l√©gumes, cens√© r√©duire les maladies chroniques.",
    "La photosynth√®se chez les plantes convertit l'√©nergie lumineuse en glucose et produit de l'oxyg√®ne essentiel.",
    "Les innovations du 20e si√®cle, des radios aux smartphones, se sont centr√©es sur les avanc√©es √©lectroniques.",
    "Les rivi√®res fournissent de l'eau, l'irrigation et l'habitat pour les esp√®ces aquatiques, vitales pour les √©cosyst√®mes.",
    "La conf√©rence t√©l√©phonique d'Apple pour discuter des r√©sultats du quatri√®me trimestre fiscal et des mises √† jour commerciales est pr√©vue pour jeudi 2 novembre 2023 √† 14h00 PT / 17h00 ET.",
    "Les ≈ìuvres de Shakespeare, comme 'Hamlet' et 'Le Songe d'une nuit d'√©t√©', perdurent dans la litt√©rature."
]

```

Nous utiliserons d'abord Voyage pour convertir chacun d'eux en un vecteur d'embedding

```python
import voyageai

vo = voyageai.Client()

# Int√©grer les documents
doc_embds = vo.embed(
    documents, model="voyage-3.5", input_type="document"
).embeddings
```

Les embeddings nous permettront de faire de la recherche / r√©cup√©ration s√©mantique dans l'espace vectoriel. √âtant donn√© un exemple de requ√™te,

```python
query = "Quand la conf√©rence t√©l√©phonique d'Apple est-elle pr√©vue ?"
```

nous la convertissons en embedding, et effectuons une recherche du plus proche voisin pour trouver le document le plus pertinent bas√© sur la distance dans l'espace d'embedding.

```python
import numpy as np

# Int√©grer la requ√™te
query_embd = vo.embed(
    [query], model="voyage-3.5", input_type="query"
).embeddings[0]

# Calculer la similarit√©
# Les embeddings Voyage sont normalis√©s √† la longueur 1, donc le produit scalaire
# et la similarit√© cosinus sont identiques.
similarities = np.dot(doc_embds, query_embd)

retrieved_id = np.argmax(similarities)
print(documents[retrieved_id])
```

Notez que nous utilisons `input_type="document"` et `input_type="query"` pour int√©grer le document et la requ√™te, respectivement. Plus de sp√©cifications peuvent √™tre trouv√©es [ici](/docs/fr/build-with-claude/embeddings#voyage-python-package).

La sortie serait le 5e document, qui est effectivement le plus pertinent pour la requ√™te :

```
La conf√©rence t√©l√©phonique d'Apple pour discuter des r√©sultats du quatri√®me trimestre fiscal et des mises √† jour commerciales est pr√©vue pour jeudi 2 novembre 2023 √† 14h00 PT / 17h00 ET.
```

Si vous cherchez un ensemble d√©taill√© de livres de recettes sur comment faire du RAG avec des embeddings, incluant les bases de donn√©es vectorielles, consultez notre [livre de recettes RAG](https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/Pinecone/rag_using_pinecone.ipynb).

## FAQ

  <section title="Pourquoi les embeddings Voyage ont-ils une qualit√© sup√©rieure ?">

    Les mod√®les d'embedding s'appuient sur de puissants r√©seaux de neurones pour capturer et compresser le contexte s√©mantique, similaire aux mod√®les g√©n√©ratifs. L'√©quipe de chercheurs en IA exp√©riment√©s de Voyage optimise chaque composant du processus d'embedding, incluant :
    - Architecture du mod√®le
    - Collecte de donn√©es
    - Fonctions de perte
    - S√©lection d'optimiseur

    Apprenez-en plus sur l'approche technique de Voyage sur leur [blog](https://blog.voyageai.com/).
  
</section>

  <section title="Quels mod√®les d'embedding sont disponibles et lequel devrais-je utiliser ?">

    Pour l'embedding g√©n√©raliste, nous recommandons :
    - `voyage-3-large` : Meilleure qualit√©
    - `voyage-3.5-lite` : Latence et co√ªt les plus bas
    - `voyage-3.5` : Performance √©quilibr√©e avec qualit√© de r√©cup√©ration sup√©rieure √† un prix comp√©titif
    
    Pour la r√©cup√©ration, utilisez le param√®tre `input_type` pour sp√©cifier si le texte est de type requ√™te ou document.

    Mod√®les sp√©cifiques au domaine :

    - T√¢ches juridiques : `voyage-law-2`
    - Code et documentation de programmation : `voyage-code-3`
    - T√¢ches li√©es √† la finance : `voyage-finance-2`
  
</section>

  <section title="Quelle fonction de similarit√© devrais-je utiliser ?">

    Vous pouvez utiliser les embeddings Voyage avec soit la similarit√© par produit scalaire, la similarit√© cosinus, ou la distance euclidienne. Une explication sur la similarit√© d'embedding peut √™tre trouv√©e [ici](https://www.pinecone.io/learn/vector-similarity/).

    Les embeddings Voyage AI sont normalis√©s √† la longueur 1, ce qui signifie que :

    - La similarit√© cosinus est √©quivalente √† la similarit√© par produit scalaire, tandis que cette derni√®re peut √™tre calcul√©e plus rapidement.
    - La similarit√© cosinus et la distance euclidienne donneront des classements identiques.
  
</section>

  <section title="Quelle est la relation entre caract√®res, mots et tokens ?">

    Veuillez voir cette [page](https://docs.voyageai.com/docs/tokenization?ref=anthropic).
  
</section>

  <section title="Quand et comment devrais-je utiliser le param√®tre input_type ?">

    Pour toutes les t√¢ches de r√©cup√©ration et cas d'usage (par exemple, RAG), nous recommandons que le param√®tre `input_type` soit utilis√© pour sp√©cifier si le texte d'entr√©e est une requ√™te ou un document. N'omettez pas `input_type` ou ne d√©finissez pas `input_type=None`. Sp√©cifier si le texte d'entr√©e est une requ√™te ou un document peut cr√©er de meilleures repr√©sentations vectorielles denses pour la r√©cup√©ration, ce qui peut conduire √† une meilleure qualit√© de r√©cup√©ration.

    Lors de l'utilisation du param√®tre `input_type`, des invites sp√©ciales sont ajout√©es au d√©but du texte d'entr√©e avant l'embedding. Sp√©cifiquement :

    > üìò **Invites associ√©es avec `input_type`**
    > 
    > - Pour une requ√™te, l'invite est "Represent the query for retrieving supporting documents: ".
    > - Pour un document, l'invite est "Represent the document for retrieval: ".
    > - Exemple
    >     - Quand `input_type="query"`, une requ√™te comme "Quand la conf√©rence t√©l√©phonique d'Apple est-elle pr√©vue ?" deviendra "**Represent the query for retrieving supporting documents:** Quand la conf√©rence t√©l√©phonique d'Apple est-elle pr√©vue ?"
    >     - Quand `input_type="document"`, une requ√™te comme "La conf√©rence t√©l√©phonique d'Apple pour discuter des r√©sultats du quatri√®me trimestre fiscal et des mises √† jour commerciales est pr√©vue pour jeudi 2 novembre 2023 √† 14h00 PT / 17h00 ET." deviendra "**Represent the document for retrieval:** La conf√©rence t√©l√©phonique d'Apple pour discuter des r√©sultats du quatri√®me trimestre fiscal et des mises √† jour commerciales est pr√©vue pour jeudi 2 novembre 2023 √† 14h00 PT / 17h00 ET."

    `voyage-large-2-instruct`, comme le nom le sugg√®re, est entra√Æn√© pour √™tre r√©actif aux instructions suppl√©mentaires qui sont ajout√©es au d√©but du texte d'entr√©e. Pour la classification, le clustering, ou d'autres sous-t√¢ches [MTEB](https://huggingface.co/mteb), veuillez utiliser les instructions [ici](https://github.com/voyage-ai/voyage-large-2-instruct).
  
</section>

  <section title="Quelles options de quantification sont disponibles ?">

    La quantification dans les embeddings convertit les valeurs haute pr√©cision, comme les nombres √† virgule flottante simple pr√©cision 32 bits, vers des formats de pr√©cision inf√©rieure tels que les entiers 8 bits ou les valeurs binaires 1 bit, r√©duisant le stockage, la m√©moire et les co√ªts de 4x et 32x, respectivement. Les mod√®les Voyage support√©s permettent la quantification en sp√©cifiant le type de donn√©es de sortie avec le param√®tre `output_dtype` :

    - `float` : Chaque embedding retourn√© est une liste de nombres √† virgule flottante simple pr√©cision 32 bits (4 octets). C'est la valeur par d√©faut et fournit la plus haute pr√©cision / pr√©cision de r√©cup√©ration.
    - `int8` et `uint8` : Chaque embedding retourn√© est une liste d'entiers 8 bits (1 octet) allant de -128 √† 127 et de 0 √† 255, respectivement.
    - `binary` et `ubinary` : Chaque embedding retourn√© est une liste d'entiers 8 bits qui repr√©sentent des valeurs d'embedding quantifi√©es sur un bit, empaquet√©es en bits : `int8` pour `binary` et `uint8` pour `ubinary`. La longueur de la liste retourn√©e d'entiers est 1/8 de la dimension r√©elle de l'embedding. Le type binaire utilise la m√©thode binaire d√©cal√©e, sur laquelle vous pouvez en apprendre plus dans la FAQ ci-dessous.

    > **Exemple de quantification binaire**
    > 
    > Consid√©rez les huit valeurs d'embedding suivantes : -0.03955078, 0.006214142, -0.07446289, -0.039001465, 0.0046463013, 0.00030612946, -0.08496094, et 0.03994751. Avec la quantification binaire, les valeurs inf√©rieures ou √©gales √† z√©ro seront quantifi√©es √† un z√©ro binaire, et les valeurs positives √† un un binaire, r√©sultant en la s√©quence binaire suivante : 0, 1, 0, 0, 1, 1, 0, 1. Ces huit bits sont ensuite empaquet√©s en un seul entier 8 bits, 01001101 (avec le bit le plus √† gauche comme bit le plus significatif).
    >   - `ubinary` : La s√©quence binaire est directement convertie et repr√©sent√©e comme l'entier non sign√© (`uint8`) 77.
    >   - `binary` : La s√©quence binaire est repr√©sent√©e comme l'entier sign√© (`int8`) -51, calcul√© en utilisant la m√©thode binaire d√©cal√©e (77 - 128 = -51).
  
</section>

  <section title="Comment puis-je tronquer les embeddings Matryoshka ?">

    L'apprentissage Matryoshka cr√©e des embeddings avec des repr√©sentations grossi√®res √† fines dans un seul vecteur. Les mod√®les Voyage, tels que `voyage-code-3`, qui supportent plusieurs dimensions de sortie g√©n√®rent de tels embeddings Matryoshka. Vous pouvez tronquer ces vecteurs en gardant le sous-ensemble principal des dimensions. Par exemple, le code Python suivant d√©montre comment tronquer des vecteurs 1024-dimensionnels √† 256 dimensions :

    ```python
    import voyageai
    import numpy as np

    def embd_normalize(v: np.ndarray) -> np.ndarray:
        """
        Normaliser les lignes d'un tableau numpy 2D en vecteurs unitaires en divisant chaque ligne par sa
        norme euclidienne. L√®ve une ValueError si une ligne a une norme de z√©ro pour √©viter la division par z√©ro.
        """
        row_norms = np.linalg.norm(v, axis=1, keepdims=True)
        if np.any(row_norms == 0):
            raise ValueError("Cannot normalize rows with a norm of zero.")
        return v / row_norms


    vo = voyageai.Client()

    # G√©n√©rer des vecteurs voyage-code-3, qui par d√©faut sont des nombres √† virgule flottante 1024-dimensionnels
    embd = vo.embed(['Texte d'exemple 1', 'Texte d'exemple 2'], model='voyage-code-3').embeddings

    # D√©finir une dimension plus courte
    short_dim = 256

    # Redimensionner et normaliser les vecteurs √† une dimension plus courte
    resized_embd = embd_normalize(np.array(embd)[:, :short_dim]).tolist()
    ```
  
</section>

## Tarification

Visitez la [page de tarification](https://docs.voyageai.com/docs/pricing?ref=anthropic) de Voyage pour les d√©tails de tarification les plus √† jour.