# Снижение задержки

Узнайте, как измерить и уменьшить задержку при работе с Claude для повышения производительности и отзывчивости приложений.

---

Задержка относится к времени, которое требуется модели для обработки запроса и генерации вывода. На задержку могут влиять различные факторы, такие как размер модели, сложность запроса и базовая инфраструктура, поддерживающая модель и точку взаимодействия.

<Note>
Всегда лучше сначала создать запрос, который хорошо работает без ограничений модели или запроса, а затем попробовать стратегии снижения задержки. Попытка преждевременно снизить задержку может помешать вам обнаружить, как выглядит максимальная производительность.
</Note>

---

## Как измерить задержку

При обсуждении задержки вы можете столкнуться с несколькими терминами и измерениями:

- **Базовая задержка**: Это время, затрачиваемое моделью на обработку запроса и генерацию ответа, без учета входных и выходных токенов в секунду. Это дает общее представление о скорости модели.
- **Время до первого токена (TTFT)**: Эта метрика измеряет время, которое требуется модели для генерации первого токена ответа с момента отправки запроса. Это особенно актуально, когда вы используете потоковую передачу (подробнее об этом позже) и хотите обеспечить отзывчивый опыт для ваших пользователей.

Для более глубокого понимания этих терминов ознакомьтесь с нашим [глоссарием](/docs/ru/about-claude/glossary).

---

## Как снизить задержку

### 1. Выберите правильную модель

Один из самых простых способов снизить задержку - это выбрать подходящую модель для вашего случая использования. Anthropic предлагает [ряд моделей](/docs/ru/about-claude/models/overview) с различными возможностями и характеристиками производительности. Рассмотрите ваши конкретные требования и выберите модель, которая лучше всего подходит для ваших потребностей с точки зрения скорости и качества вывода.

Для критически важных по скорости приложений **Claude Haiku 4.5** предлагает самое быстрое время отклика при сохранении высокого интеллекта:

```python
import anthropic

client = anthropic.Anthropic()

# Для чувствительных ко времени приложений используйте Claude Haiku 4.5
message = client.messages.create(
    model="claude-haiku-4-5",
    max_tokens=100,
    messages=[{
        "role": "user",
        "content": "Summarize this customer feedback in 2 sentences: [feedback text]"
    }]
)
```

Для получения более подробной информации о метриках моделей см. нашу страницу [обзор моделей](/docs/ru/about-claude/models/overview).

### 2. Оптимизируйте длину запроса и вывода

Минимизируйте количество токенов как в вашем входном запросе, так и в ожидаемом выводе, сохраняя при этом высокую производительность. Чем меньше токенов модель должна обработать и сгенерировать, тем быстрее будет ответ.

Вот несколько советов, которые помогут вам оптимизировать ваши запросы и выводы:

- **Будьте ясными, но краткими**: Стремитесь четко и кратко передать ваше намерение в запросе. Избегайте ненужных деталей или избыточной информации, помня при этом, что [claude не имеет контекста](/docs/ru/build-with-claude/prompt-engineering/be-clear-and-direct) о вашем случае использования и может не сделать предполагаемые логические скачки, если инструкции неясны.
- **Просите более короткие ответы**: Попросите Claude напрямую быть кратким. Семейство моделей Claude 3 имеет улучшенную управляемость по сравнению с предыдущими поколениями. Если Claude выдает нежелательную длину, попросите Claude [обуздать свою болтливость](/docs/ru/build-with-claude/prompt-engineering/be-clear-and-direct).
  <Tip> Из-за того, как LLM подсчитывают [токены](/docs/ru/about-claude/glossary#tokens) вместо слов, просьба о точном количестве слов или ограничении количества слов не является такой эффективной стратегией, как просьба об ограничениях количества абзацев или предложений.</Tip>
- **Установите соответствующие ограничения вывода**: Используйте параметр `max_tokens`, чтобы установить жесткое ограничение на максимальную длину сгенерированного ответа. Это предотвращает генерацию Claude слишком длинных выводов.
  > **Примечание**: Когда ответ достигает `max_tokens` токенов, ответ будет обрезан, возможно, посреди предложения или посреди слова, поэтому это грубая техника, которая может потребовать постобработки и обычно наиболее подходит для множественного выбора или коротких ответов, где ответ приходит прямо в начале.
- **Экспериментируйте с температурой**: [Параметр](/docs/ru/api/messages) `temperature` контролирует случайность вывода. Более низкие значения (например, 0.2) иногда могут привести к более сфокусированным и коротким ответам, в то время как более высокие значения (например, 0.8) могут привести к более разнообразным, но потенциально более длинным выводам.

Поиск правильного баланса между ясностью запроса, качеством вывода и количеством токенов может потребовать некоторых экспериментов.

### 3. Используйте потоковую передачу

Потоковая передача - это функция, которая позволяет модели начать отправлять свой ответ до того, как полный вывод будет завершен. Это может значительно улучшить воспринимаемую отзывчивость вашего приложения, поскольку пользователи могут видеть вывод модели в реальном времени.

С включенной потоковой передачей вы можете обрабатывать вывод модели по мере его поступления, обновляя пользовательский интерфейс или выполняя другие задачи параллельно. Это может значительно улучшить пользовательский опыт и сделать ваше приложение более интерактивным и отзывчивым.

Посетите [потоковые сообщения](/docs/ru/build-with-claude/streaming), чтобы узнать о том, как вы можете реализовать потоковую передачу для вашего случая использования.