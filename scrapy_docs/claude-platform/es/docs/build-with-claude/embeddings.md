# Embeddings

Los embeddings de texto son representaciones num칠ricas del texto que permiten medir la similitud sem치ntica. Esta gu칤a introduce los embeddings, sus aplicaciones y c칩mo usar modelos de embedding para tareas como b칰squeda, recomendaciones y detecci칩n de anomal칤as.

---

## Antes de implementar embeddings

Al seleccionar un proveedor de embeddings, hay varios factores que puedes considerar dependiendo de tus necesidades y preferencias:

- Tama침o del conjunto de datos y especificidad del dominio: tama침o del conjunto de datos de entrenamiento del modelo y su relevancia para el dominio que deseas embebir. Los datos m치s grandes o m치s espec칤ficos del dominio generalmente producen mejores embeddings dentro del dominio
- Rendimiento de inferencia: velocidad de b칰squeda de embeddings y latencia de extremo a extremo. Esta es una consideraci칩n particularmente importante para implementaciones de producci칩n a gran escala
- Personalizaci칩n: opciones para entrenamiento continuo en datos privados, o especializaci칩n de modelos para dominios muy espec칤ficos. Esto puede mejorar el rendimiento en vocabularios 칰nicos

## C칩mo obtener embeddings con Anthropic

Anthropic no ofrece su propio modelo de embedding. Un proveedor de embeddings que tiene una amplia variedad de opciones y capacidades que abarcan todas las consideraciones anteriores es Voyage AI.

Voyage AI crea modelos de embedding de vanguardia y ofrece modelos personalizados para dominios industriales espec칤ficos como finanzas y atenci칩n m칠dica, o modelos ajustados a medida para clientes individuales.

El resto de esta gu칤a es para Voyage AI, pero te animamos a evaluar una variedad de proveedores de embeddings para encontrar el mejor ajuste para tu caso de uso espec칤fico.

## Modelos Disponibles

Voyage recomienda usar los siguientes modelos de embedding de texto:

| Modelo | Longitud de Contexto | Dimensi칩n de Embedding | Descripci칩n |
| --- | --- | --- | --- |
| `voyage-3-large` | 32,000 | 1024 (predeterminado), 256, 512, 2048 | La mejor calidad de recuperaci칩n general y multiling칲e. Ver [publicaci칩n del blog](https://blog.voyageai.com/2025/01/07/voyage-3-large/) para detalles. |
| `voyage-3.5` | 32,000 | 1024 (predeterminado), 256, 512, 2048 | Optimizado para calidad de recuperaci칩n general y multiling칲e. Ver [publicaci칩n del blog](https://blog.voyageai.com/2025/05/20/voyage-3-5/) para detalles. |
| `voyage-3.5-lite` | 32,000 | 1024 (predeterminado), 256, 512, 2048 | Optimizado para latencia y costo. Ver [publicaci칩n del blog](https://blog.voyageai.com/2025/05/20/voyage-3-5/) para detalles. |
| `voyage-code-3` | 32,000 | 1024 (predeterminado), 256, 512, 2048 | Optimizado para recuperaci칩n de **c칩digo**. Ver [publicaci칩n del blog](https://blog.voyageai.com/2024/12/04/voyage-code-3/) para detalles. |
| `voyage-finance-2` | 32,000 | 1024 | Optimizado para recuperaci칩n y RAG de **finanzas**. Ver [publicaci칩n del blog](https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/) para detalles. |
| `voyage-law-2` | 16,000 | 1024 | Optimizado para recuperaci칩n y RAG **legal** y de **contexto largo**. Tambi칠n mejor칩 el rendimiento en todos los dominios. Ver [publicaci칩n del blog](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/) para detalles. |

Adicionalmente, se recomiendan los siguientes modelos de embedding multimodal:

| Modelo | Longitud de Contexto | Dimensi칩n de Embedding | Descripci칩n |
| --- | --- | --- | --- |
| `voyage-multimodal-3` | 32000 | 1024 | Modelo de embedding multimodal rico que puede vectorizar texto intercalado e im치genes ricas en contenido, como capturas de pantalla de PDFs, diapositivas, tablas, figuras y m치s. Ver [publicaci칩n del blog](https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/) para detalles. |

쯅ecesitas ayuda para decidir qu칠 modelo de embedding de texto usar? Consulta las [FAQ](https://docs.voyageai.com/docs/faq#what-embedding-models-are-available-and-which-one-should-i-use&ref=anthropic).

## Comenzando con Voyage AI

Para acceder a los embeddings de Voyage:

1. Reg칤strate en el sitio web de Voyage AI
2. Obt칠n una clave API
3. Establece la clave API como una variable de entorno para conveniencia:

```bash
export VOYAGE_API_KEY="<tu clave secreta>"
```

Puedes obtener los embeddings usando el paquete oficial de Python [`voyageai`](https://github.com/voyage-ai/voyageai-python) o solicitudes HTTP, como se describe a continuaci칩n.

### Biblioteca Python de Voyage

El paquete `voyageai` se puede instalar usando el siguiente comando:

```bash
pip install -U voyageai
```

Luego, puedes crear un objeto cliente y comenzar a usarlo para embebir tus textos:

```python
import voyageai

vo = voyageai.Client()
# Esto usar치 autom치ticamente la variable de entorno VOYAGE_API_KEY.
# Alternativamente, puedes usar vo = voyageai.Client(api_key="<tu clave secreta>")

texts = ["Texto de muestra 1", "Texto de muestra 2"]

result = vo.embed(texts, model="voyage-3.5", input_type="document")
print(result.embeddings[0])
print(result.embeddings[1])
```

`result.embeddings` ser치 una lista de dos vectores de embedding, cada uno conteniendo 1024 n칰meros de punto flotante. Despu칠s de ejecutar el c칩digo anterior, los dos embeddings se imprimir치n en la pantalla:

```
[-0.013131560757756233, 0.019828535616397858, ...]   # embedding para "Texto de muestra 1"
[-0.0069352793507277966, 0.020878976210951805, ...]  # embedding para "Texto de muestra 2"
```

Al crear los embeddings, puedes especificar algunos otros argumentos para la funci칩n `embed()`.

Para m치s informaci칩n sobre el paquete Python de Voyage, consulta [la documentaci칩n de Voyage](https://docs.voyageai.com/docs/embeddings#python-api).

### API HTTP de Voyage

Tambi칠n puedes obtener embeddings solicitando la API HTTP de Voyage. Por ejemplo, puedes enviar una solicitud HTTP a trav칠s del comando `curl` en una terminal:

```bash
curl https://api.voyageai.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $VOYAGE_API_KEY" \
  -d '{
    "input": ["Texto de muestra 1", "Texto de muestra 2"],
    "model": "voyage-3.5"
  }'
```

La respuesta que obtendr칤as es un objeto JSON que contiene los embeddings y el uso de tokens:

```json
{
  "object": "list",
  "data": [
    {
      "embedding": [-0.013131560757756233, 0.019828535616397858, ...],
      "index": 0
    },
    {
      "embedding": [-0.0069352793507277966, 0.020878976210951805, ...],
      "index": 1
    }
  ],
  "model": "voyage-3.5",
  "usage": {
    "total_tokens": 10
  }
}

```

Para m치s informaci칩n sobre la API HTTP de Voyage, consulta [la documentaci칩n de Voyage](https://docs.voyageai.com/reference/embeddings-api).

### AWS Marketplace

Los embeddings de Voyage est치n disponibles en [AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-snt4gb6fd7ljg). Las instrucciones para acceder a Voyage en AWS est치n disponibles [aqu칤](https://docs.voyageai.com/docs/aws-marketplace-model-package?ref=anthropic).

## Ejemplo de inicio r치pido

Ahora que sabemos c칩mo obtener embeddings, veamos un breve ejemplo.

Supongamos que tenemos un peque침o corpus de seis documentos de los cuales recuperar

```python
documents = [
    "La dieta mediterr치nea enfatiza el pescado, el aceite de oliva y las verduras, se cree que reduce las enfermedades cr칩nicas.",
    "La fotos칤ntesis en las plantas convierte la energ칤a lum칤nica en glucosa y produce ox칤geno esencial.",
    "Las innovaciones del siglo XX, desde radios hasta tel칠fonos inteligentes, se centraron en avances electr칩nicos.",
    "Los r칤os proporcionan agua, irrigaci칩n y h치bitat para especies acu치ticas, vitales para los ecosistemas.",
    "La llamada de conferencia de Apple para discutir los resultados del cuarto trimestre fiscal y actualizaciones comerciales est치 programada para el jueves 2 de noviembre de 2023 a las 2:00 p.m. PT / 5:00 p.m. ET.",
    "Las obras de Shakespeare, como 'Hamlet' y 'Sue침o de una noche de verano,' perduran en la literatura."
]

```

Primero usaremos Voyage para convertir cada uno de ellos en un vector de embedding

```python
import voyageai

vo = voyageai.Client()

# Embebir los documentos
doc_embds = vo.embed(
    documents, model="voyage-3.5", input_type="document"
).embeddings
```

Los embeddings nos permitir치n hacer b칰squeda sem치ntica / recuperaci칩n en el espacio vectorial. Dada una consulta de ejemplo,

```python
query = "쮺u치ndo est치 programada la llamada de conferencia de Apple?"
```

la convertimos en un embedding, y realizamos una b칰squeda de vecino m치s cercano para encontrar el documento m치s relevante basado en la distancia en el espacio de embedding.

```python
import numpy as np

# Embebir la consulta
query_embd = vo.embed(
    [query], model="voyage-3.5", input_type="query"
).embeddings[0]

# Calcular la similitud
# Los embeddings de Voyage est치n normalizados a longitud 1, por lo tanto el producto punto
# y la similitud coseno son lo mismo.
similarities = np.dot(doc_embds, query_embd)

retrieved_id = np.argmax(similarities)
print(documents[retrieved_id])
```

Nota que usamos `input_type="document"` e `input_type="query"` para embebir el documento y la consulta, respectivamente. M치s especificaci칩n se puede encontrar [aqu칤](/docs/es/build-with-claude/embeddings#voyage-python-package).

La salida ser칤a el 5췈 documento, que es efectivamente el m치s relevante para la consulta:

```
La llamada de conferencia de Apple para discutir los resultados del cuarto trimestre fiscal y actualizaciones comerciales est치 programada para el jueves 2 de noviembre de 2023 a las 2:00 p.m. PT / 5:00 p.m. ET.
```

Si est치s buscando un conjunto detallado de libros de cocina sobre c칩mo hacer RAG con embeddings, incluyendo bases de datos vectoriales, consulta nuestro [libro de cocina RAG](https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/Pinecone/rag_using_pinecone.ipynb).

## FAQ

  <section title="쯇or qu칠 los embeddings de Voyage tienen calidad superior?">

    Los modelos de embedding dependen de redes neuronales poderosas para capturar y comprimir el contexto sem치ntico, similar a los modelos generativos. El equipo de investigadores de IA experimentados de Voyage optimiza cada componente del proceso de embedding, incluyendo:
    - Arquitectura del modelo
    - Recolecci칩n de datos
    - Funciones de p칠rdida
    - Selecci칩n de optimizador

    Aprende m치s sobre el enfoque t칠cnico de Voyage en su [blog](https://blog.voyageai.com/).
  
</section>

  <section title="쯈u칠 modelos de embedding est치n disponibles y cu치l deber칤a usar?">

    Para embedding de prop칩sito general, recomendamos:
    - `voyage-3-large`: Mejor calidad
    - `voyage-3.5-lite`: Menor latencia y costo
    - `voyage-3.5`: Rendimiento equilibrado con calidad de recuperaci칩n superior a un punto de precio competitivo
    
    Para recuperaci칩n, usa el par치metro `input_type` para especificar si el texto es de tipo consulta o documento.

    Modelos espec칤ficos de dominio:

    - Tareas legales: `voyage-law-2`
    - C칩digo y documentaci칩n de programaci칩n: `voyage-code-3`
    - Tareas relacionadas con finanzas: `voyage-finance-2`
  
</section>

  <section title="쯈u칠 funci칩n de similitud deber칤a usar?">

    Puedes usar embeddings de Voyage con similitud de producto punto, similitud coseno o distancia euclidiana. Una explicaci칩n sobre similitud de embedding se puede encontrar [aqu칤](https://www.pinecone.io/learn/vector-similarity/).

    Los embeddings de Voyage AI est치n normalizados a longitud 1, lo que significa que:

    - La similitud coseno es equivalente a la similitud de producto punto, mientras que la 칰ltima se puede calcular m치s r치pidamente.
    - La similitud coseno y la distancia euclidiana resultar치n en clasificaciones id칠nticas.
  
</section>

  <section title="쮺u치l es la relaci칩n entre caracteres, palabras y tokens?">

    Por favor consulta esta [p치gina](https://docs.voyageai.com/docs/tokenization?ref=anthropic).
  
</section>

  <section title="쮺u치ndo y c칩mo deber칤a usar el par치metro input_type?">

    Para todas las tareas de recuperaci칩n y casos de uso (ej., RAG), recomendamos que el par치metro `input_type` se use para especificar si el texto de entrada es una consulta o documento. No omitas `input_type` o establezcas `input_type=None`. Especificar si el texto de entrada es una consulta o documento puede crear mejores representaciones vectoriales densas para recuperaci칩n, lo que puede llevar a mejor calidad de recuperaci칩n.

    Al usar el par치metro `input_type`, se anteponen prompts especiales al texto de entrada antes del embedding. Espec칤ficamente:

    > 游닂 **Prompts asociados con `input_type`**
    > 
    > - Para una consulta, el prompt es "Representa la consulta para recuperar documentos de apoyo: ".
    > - Para un documento, el prompt es "Representa el documento para recuperaci칩n: ".
    > - Ejemplo
    >     - Cuando `input_type="query"`, una consulta como "쮺u치ndo est치 programada la llamada de conferencia de Apple?" se convertir치 en "**Representa la consulta para recuperar documentos de apoyo:** 쮺u치ndo est치 programada la llamada de conferencia de Apple?"
    >     - Cuando `input_type="document"`, una consulta como "La llamada de conferencia de Apple para discutir los resultados del cuarto trimestre fiscal y actualizaciones comerciales est치 programada para el jueves 2 de noviembre de 2023 a las 2:00 p.m. PT / 5:00 p.m. ET." se convertir치 en "**Representa el documento para recuperaci칩n:** La llamada de conferencia de Apple para discutir los resultados del cuarto trimestre fiscal y actualizaciones comerciales est치 programada para el jueves 2 de noviembre de 2023 a las 2:00 p.m. PT / 5:00 p.m. ET."

    `voyage-large-2-instruct`, como sugiere el nombre, est치 entrenado para ser responsivo a instrucciones adicionales que se anteponen al texto de entrada. Para clasificaci칩n, agrupamiento u otras subtareas de [MTEB](https://huggingface.co/mteb), por favor usa las instrucciones [aqu칤](https://github.com/voyage-ai/voyage-large-2-instruct).
  
</section>

  <section title="쯈u칠 opciones de cuantizaci칩n est치n disponibles?">

    La cuantizaci칩n en embeddings convierte valores de alta precisi칩n, como n칰meros de punto flotante de precisi칩n simple de 32 bits, a formatos de menor precisi칩n como enteros de 8 bits o valores binarios de 1 bit, reduciendo el almacenamiento, memoria y costos en 4x y 32x, respectivamente. Los modelos de Voyage compatibles habilitan la cuantizaci칩n especificando el tipo de datos de salida con el par치metro `output_dtype`:

    - `float`: Cada embedding devuelto es una lista de n칰meros de punto flotante de precisi칩n simple de 32 bits (4 bytes). Este es el predeterminado y proporciona la mayor precisi칩n / exactitud de recuperaci칩n.
    - `int8` y `uint8`: Cada embedding devuelto es una lista de enteros de 8 bits (1 byte) que van de -128 a 127 y de 0 a 255, respectivamente.
    - `binary` y `ubinary`: Cada embedding devuelto es una lista de enteros de 8 bits que representan valores de embedding cuantizados de un solo bit empaquetados en bits: `int8` para `binary` y `uint8` para `ubinary`. La longitud de la lista devuelta de enteros es 1/8 de la dimensi칩n real del embedding. El tipo binario usa el m칠todo binario de desplazamiento, sobre el cual puedes aprender m치s en las FAQ a continuaci칩n.

    > **Ejemplo de cuantizaci칩n binaria**
    > 
    > Considera los siguientes ocho valores de embedding: -0.03955078, 0.006214142, -0.07446289, -0.039001465, 0.0046463013, 0.00030612946, -0.08496094, y 0.03994751. Con cuantizaci칩n binaria, los valores menores o iguales a cero ser치n cuantizados a un cero binario, y los valores positivos a un uno binario, resultando en la siguiente secuencia binaria: 0, 1, 0, 0, 1, 1, 0, 1. Estos ocho bits se empaquetan luego en un solo entero de 8 bits, 01001101 (con el bit m치s a la izquierda como el bit m치s significativo).
    >   - `ubinary`: La secuencia binaria se convierte directamente y se representa como el entero sin signo (`uint8`) 77.
    >   - `binary`: La secuencia binaria se representa como el entero con signo (`int8`) -51, calculado usando el m칠todo binario de desplazamiento (77 - 128 = -51).
  
</section>

  <section title="쮺칩mo puedo truncar embeddings Matryoshka?">

    El aprendizaje Matryoshka crea embeddings con representaciones de grueso a fino dentro de un solo vector. Los modelos de Voyage, como `voyage-code-3`, que soportan m칰ltiples dimensiones de salida generan tales embeddings Matryoshka. Puedes truncar estos vectores manteniendo el subconjunto principal de dimensiones. Por ejemplo, el siguiente c칩digo Python demuestra c칩mo truncar vectores de 1024 dimensiones a 256 dimensiones:

    ```python
    import voyageai
    import numpy as np

    def embd_normalize(v: np.ndarray) -> np.ndarray:
        """
        Normaliza las filas de un array numpy 2D a vectores unitarios dividiendo cada fila por su
        norma euclidiana. Lanza un ValueError si alguna fila tiene una norma de cero para prevenir divisi칩n por cero.
        """
        row_norms = np.linalg.norm(v, axis=1, keepdims=True)
        if np.any(row_norms == 0):
            raise ValueError("No se pueden normalizar filas con una norma de cero.")
        return v / row_norms


    vo = voyageai.Client()

    # Generar vectores voyage-code-3, que por defecto son n칰meros de punto flotante de 1024 dimensiones
    embd = vo.embed(['Texto de muestra 1', 'Texto de muestra 2'], model='voyage-code-3').embeddings

    # Establecer dimensi칩n m치s corta
    short_dim = 256

    # Redimensionar y normalizar vectores a dimensi칩n m치s corta
    resized_embd = embd_normalize(np.array(embd)[:, :short_dim]).tolist()
    ```
  
</section>

## Precios

Visita la [p치gina de precios](https://docs.voyageai.com/docs/pricing?ref=anthropic) de Voyage para los detalles de precios m치s actualizados.